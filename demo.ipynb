{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "DEFAULT = None\n",
    "CPU = 'cpu'\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, array:np.ndarray, _children=(), _op='', label=''):\n",
    "        self.shape = array.shape\n",
    "        self.array = array\n",
    "        self.grad = np.zeros(self.shape, dtype=np.float32)\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "    def __repr__(self) -> str:\n",
    "        # return f\"Value(shape={self.array.shape}, grad={self.grad.shape})\"\n",
    "        return self.array.__repr__()\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.array + other.array, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += np.ones_like(self.array) * out.grad\n",
    "            if other.shape != out.shape:\n",
    "                other.grad += (np.ones_like(other.array) * out.grad).sum(axis=0)\n",
    "            else:\n",
    "                other.grad += np.ones_like(other.array) * out.grad\n",
    "            # self.grad += 1.0 * out.grad\n",
    "            # other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    def __neg__(self): return self * (-1)\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    def __rsub__(self, other):\n",
    "        return other + (-self)\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, np.ndarray) else -np.ones_like(self.array)\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.array * other.array, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.array * out.grad\n",
    "            other.grad += self.array * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    def __rmul__(self, other): return self * other\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supports int/float powers for now\"\n",
    "        out = Tensor(self.array ** other, (self, ), f'**{other}')\n",
    "        def _backward():\n",
    "            self.grad += other * (self.array ** (other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def dot(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        assert len(other.shape) == 1, \"support only one dimension use mm instead\"\n",
    "        ...\n",
    "    def mm(self, other):\n",
    "        assert len(other.shape) == 2, \"not support higher dimensions then matrix\"\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.array.dot(other.array), (self, other), 'mm')\n",
    "        def _backward():\n",
    "            # self.grad += np.tile(other.array.sum(axis=-1), (self.shape[-2], 1))# * out.grad\n",
    "            self.grad += np.dot(out.grad, other.array.T)\n",
    "            # other.grad += np.tile(self.array.sum(axis=-2), (other.shape[-1], 1)).T #* out.grad\n",
    "            other.grad = np.dot(self.array.T, out.grad)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    # def matmul\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited: \n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        \n",
    "        self.grad = np.ones(self.shape)\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "    def exp(self):\n",
    "        x = self.array\n",
    "        out = Tensor(np.exp(x), (self, ), 'exp')\n",
    "        def _backward():\n",
    "            self.grad += out.array * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    def mean(self, axis=...):\n",
    "        x = self.array\n",
    "        out = Tensor(x.mean(axis=axis), (self, ), 'mean')\n",
    "        def _backward():\n",
    "            self.grad += out.grad/self.shape[0]\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    def sum(self, axis=...):\n",
    "        x = self.array\n",
    "        out = Tensor(x.sum(axis=axis), (self, ), 'sum')\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    def step(self, lr): \n",
    "        lr = np.array([lr], dtype=np.float32)\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited: \n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        for node in reversed(topo):\n",
    "            node.array -= lr * node.grad\n",
    "    def zero_grad(self): \n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited: \n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        for node in reversed(topo):\n",
    "            node.grad *= 0\n",
    "    def item(self):\n",
    "        assert len(self.shape) == 1 and self.shape[0] == 1\n",
    "        return self.array[0]\n",
    "    \n",
    "class Loss: ...\n",
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        ...\n",
    "    def __call__(self, y:Tensor, y_real:Tensor, axis=0):\n",
    "        return ((y - y_real)**2).mean(axis=axis)\n",
    "\n",
    "class Optimizer: ...\n",
    "class Adam: ...\n",
    "class SGD: ... # StochasticGradDescent\n",
    "\n",
    "\n",
    "class Activation: ...\n",
    "class ReLU(Activation): ...\n",
    "\n",
    "class Sigmoid(Activation): \n",
    "    def __init__(self): \n",
    "        self.grad = None\n",
    "    def __call__(self, x:Tensor):\n",
    "        return self.forward(x)\n",
    "    def forward(self, x:Tensor):\n",
    "        self.grad = self.backward(x)\n",
    "        # return 1/(1+np.exp(-x))\n",
    "        return (Tensor(np.ones_like(x.array))+(-x).exp())**(-1)\n",
    "    \n",
    "    def backward(self, x):\n",
    "        ...\n",
    "        # return x * (1-x)\n",
    "\n",
    "class Module:\n",
    "    def __init__(self): ...\n",
    "    def __call__(self): ...\n",
    "class Linear:\n",
    "    def __init__(self, in_features, out_features, bias=True, device=DEFAULT, dtype=DEFAULT):\n",
    "        self.dtype = dtype if dtype is not None else np.float32\n",
    "        self.device = device if device is not None else CPU\n",
    "        \n",
    "        self.W = Tensor(np.random.uniform(-1, 1, (in_features, out_features)).astype(self.dtype)) # no need to transpose the tensors as i am initialized here with Transpose shape\n",
    "        # self.W = np.random.random((in_features, out_features)).astype(self.dtype)\n",
    "        self.B = Tensor(np.zeros(out_features, dtype=self.dtype))\n",
    "    def __call__(self, X:Tensor):\n",
    "        return self.forward(X)\n",
    "    def forward(self, x:Tensor):\n",
    "        return x.mm(self.W) + self.B\n",
    "        # return np.dot(x, self.W) + self.B\n",
    "    def backward(self, x):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel:\n",
    "    def __init__(self) -> None:\n",
    "        self.fc1 = Linear(30, 10)\n",
    "        self.fn1 = Sigmoid()\n",
    "        self.fc2 = Linear(10, 1)\n",
    "        self.fn2 = Sigmoid()\n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fn1(x)\n",
    "        x = self.fc2(x)\n",
    "        # return x\n",
    "        return self.fn2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 | LOSS: 0.1732\n",
      "EPOCH: 100 | LOSS: 0.1672\n",
      "EPOCH: 200 | LOSS: 0.1614\n",
      "EPOCH: 300 | LOSS: 0.1561\n",
      "EPOCH: 400 | LOSS: 0.1510\n",
      "EPOCH: 500 | LOSS: 0.1463\n",
      "EPOCH: 600 | LOSS: 0.1418\n",
      "EPOCH: 700 | LOSS: 0.1376\n",
      "EPOCH: 800 | LOSS: 0.1336\n",
      "EPOCH: 900 | LOSS: 0.1299\n",
      "EPOCH: 1000 | LOSS: 0.1263\n",
      "EPOCH: 1100 | LOSS: 0.1229\n",
      "EPOCH: 1200 | LOSS: 0.1197\n",
      "EPOCH: 1300 | LOSS: 0.1167\n",
      "EPOCH: 1400 | LOSS: 0.1138\n",
      "EPOCH: 1500 | LOSS: 0.1111\n",
      "EPOCH: 1600 | LOSS: 0.1085\n",
      "EPOCH: 1700 | LOSS: 0.1060\n",
      "EPOCH: 1800 | LOSS: 0.1037\n",
      "EPOCH: 1900 | LOSS: 0.1014\n",
      "EPOCH: 2000 | LOSS: 0.0993\n",
      "EPOCH: 2100 | LOSS: 0.0972\n",
      "EPOCH: 2200 | LOSS: 0.0952\n",
      "EPOCH: 2300 | LOSS: 0.0933\n",
      "EPOCH: 2400 | LOSS: 0.0915\n",
      "EPOCH: 2500 | LOSS: 0.0898\n",
      "EPOCH: 2600 | LOSS: 0.0881\n",
      "EPOCH: 2700 | LOSS: 0.0865\n",
      "EPOCH: 2800 | LOSS: 0.0849\n",
      "EPOCH: 2900 | LOSS: 0.0834\n",
      "EPOCH: 3000 | LOSS: 0.0820\n",
      "EPOCH: 3100 | LOSS: 0.0806\n",
      "EPOCH: 3200 | LOSS: 0.0792\n",
      "EPOCH: 3300 | LOSS: 0.0779\n",
      "EPOCH: 3400 | LOSS: 0.0767\n",
      "EPOCH: 3500 | LOSS: 0.0755\n",
      "EPOCH: 3600 | LOSS: 0.0743\n",
      "EPOCH: 3700 | LOSS: 0.0732\n",
      "EPOCH: 3800 | LOSS: 0.0721\n",
      "EPOCH: 3900 | LOSS: 0.0710\n",
      "EPOCH: 4000 | LOSS: 0.0699\n",
      "EPOCH: 4100 | LOSS: 0.0689\n",
      "EPOCH: 4200 | LOSS: 0.0680\n",
      "EPOCH: 4300 | LOSS: 0.0670\n",
      "EPOCH: 4400 | LOSS: 0.0661\n",
      "EPOCH: 4500 | LOSS: 0.0652\n",
      "EPOCH: 4600 | LOSS: 0.0643\n",
      "EPOCH: 4700 | LOSS: 0.0635\n",
      "EPOCH: 4800 | LOSS: 0.0626\n",
      "EPOCH: 4900 | LOSS: 0.0618\n",
      "Accuracy on test data: 0.9474\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Load real data (Breast Cancer Wisconsin dataset)\n",
    "data = load_breast_cancer()\n",
    "X, y_real = data.data.astype(np.float32), data.target.astype(np.float32)\n",
    "y_real = y_real.reshape(-1, 1)\n",
    "\n",
    "# Step 2: Preprocess data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_real, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = Tensor(X_train), Tensor(X_test), Tensor(y_train), Tensor(y_test)\n",
    "\n",
    "model = TestModel()\n",
    "criterion = MSELoss()\n",
    "for epoch in range(5000):\n",
    "    y = model(X_train)    \n",
    "    loss = criterion(y, y_train)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"EPOCH: {epoch} | LOSS: {loss.item():.4f}\")\n",
    "    loss.backward()\n",
    "    loss.step(0.003)\n",
    "    loss.zero_grad()\n",
    "\n",
    "    \n",
    "# Step 5: Evaluate model\n",
    "outputs = model(X_test)\n",
    "preds = outputs.array\n",
    "\n",
    "accuracy = (np.round(preds) == y_test.array).mean()\n",
    "print(f'Accuracy on test data: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
